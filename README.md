# 序列标注编程作业：基于Transformer的命名实体识别（NER）

## 实验简介
本项目是一个序列标注编程作业，目标是设计并实现一个基于Transformer模型的命名实体识别（NER）系统。该系统将通过训练语料学习识别文本中的实体，并为其分配正确的标签。使用BERT预训练模型进行模型的初始化和微调，以提高模型在NER任务上的性能。

## 实验目的
- 理解和掌握Transformer模型及其在序列标注任务中的应用。
- 学习如何使用BERT预训练模型进行NER任务的微调。
- 分析模型性能并优化模型结构和参数。

## 实验环境
- 编程语言：Python 3.9
- 依赖库：PyTorch, Transformers, BERT tokenizer
- 硬件环境：推荐使用GPU加速训练

## 数据准备
- 训练语料：`train.txt` 和 `train_TAG.txt`
- 发展集：`dev.txt` 和 `dev_TAG.txt`
- 测试语料：`test.txt`

## 实验步骤
1. **数据准备**：从训练标签文件中统计标签集合，并创建标签到索引的映射。
2. **加载BERT预训练模型**：使用BERT模型进行序列标注任务。
3. **模型微调**：在训练集上进行模型微调，并在发展集上评估模型性能。
4. **损失曲线和性能变化曲线**：记录训练过程中的损失和准确率变化。
5. **保存和重新加载模型参数**：保存训练好的模型参数并在需要时重新加载。
6. **BERT+CRF尝试**：尝试将CRF层添加到BERT模型中，以提高序列标注的性能。

## 提交文件
- 实验报告：包含模型设计、训练细节、性能曲线和部分代码实现。
- 代码：包含数据处理、模型定义、训练和评估的完整实现代码。
- 标注文件：对`test.txt`进行序列标注得到的标注文件，命名方式为`学号.txt`。

## 实验结果
实验结果显示，BERT模型在NER任务上取得了较高的准确率。通过微调和结构调整，模型性能得到了进一步提升。

## 注意事项
- 所有输出文本均采用Unicode(UTF-8)编码。
- 提交的代码需要有详细的注释说明，关键部分应与实验报告中的参数和执行细节对应。

## 参考资料
- 本次实验使用了BERT预训练模型，以及PyTorch和Transformers库。
