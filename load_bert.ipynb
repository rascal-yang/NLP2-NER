{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForTokenClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = []\n",
    "# 打开文件并逐行读取\n",
    "with open('tag_set.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 分割每一行中的标签 再通过空格分隔\n",
    "        tags = line.strip().split()\n",
    "        # 更新集合，自动去除重复的标签\n",
    "        tag_set += tags\n",
    "tag_set += ['pad']\n",
    "\n",
    "num_labels = len(tag_set)\n",
    "\n",
    "# 创建标签到索引的映射\n",
    "label2idx = {label: idx for idx, label in enumerate(tag_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "示例文本： 记者1月15日从上海铁路局淮南西站获悉,淮南铁路预计春运期间发送旅客33.3万人。预计客流最高峰日为2月6日,将发送旅客1.8万人,淮南东开往广州南及北京方向的高铁、淮南站开往沪杭甬方向的列车将是热门车次。\n"
     ]
    }
   ],
   "source": [
    "test_texts = []\n",
    "\n",
    "# 读取文本文件\n",
    "with open('data/test.txt', 'r', encoding='utf-8') as file_texts:\n",
    "    test_texts = [line.strip().replace(' ', '') for line in file_texts if line.strip()]\n",
    "\n",
    "print(\"示例文本：\", test_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均长度: 83.4152832774901\n",
      "最短长度: 1, 最长长度: 1457\n",
      "长度频率分布:\n",
      "长度 1: 出现次数 41\n",
      "长度 2: 出现次数 206\n",
      "长度 3: 出现次数 148\n",
      "长度 4: 出现次数 275\n",
      "长度 5: 出现次数 326\n",
      "长度 6: 出现次数 294\n",
      "长度 7: 出现次数 332\n",
      "长度 8: 出现次数 349\n",
      "长度 9: 出现次数 335\n",
      "长度 10: 出现次数 364\n",
      "长度 11: 出现次数 375\n",
      "长度 12: 出现次数 368\n",
      "长度 13: 出现次数 349\n",
      "长度 14: 出现次数 323\n",
      "长度 15: 出现次数 294\n",
      "长度 16: 出现次数 252\n",
      "长度 17: 出现次数 316\n",
      "长度 18: 出现次数 243\n",
      "长度 19: 出现次数 419\n",
      "长度 20: 出现次数 362\n",
      "长度 21: 出现次数 407\n",
      "长度 22: 出现次数 435\n",
      "长度 23: 出现次数 274\n",
      "长度 24: 出现次数 290\n",
      "长度 25: 出现次数 255\n",
      "长度 26: 出现次数 229\n",
      "长度 27: 出现次数 224\n",
      "长度 28: 出现次数 162\n",
      "长度 29: 出现次数 108\n",
      "长度 30: 出现次数 134\n",
      "长度 31: 出现次数 91\n",
      "长度 32: 出现次数 92\n",
      "长度 33: 出现次数 101\n",
      "长度 34: 出现次数 105\n",
      "长度 35: 出现次数 83\n",
      "长度 36: 出现次数 88\n",
      "长度 37: 出现次数 83\n",
      "长度 38: 出现次数 69\n",
      "长度 39: 出现次数 79\n",
      "长度 40: 出现次数 104\n",
      "长度 41: 出现次数 91\n",
      "长度 42: 出现次数 103\n",
      "长度 43: 出现次数 74\n",
      "长度 44: 出现次数 99\n",
      "长度 45: 出现次数 128\n",
      "长度 46: 出现次数 95\n",
      "长度 47: 出现次数 143\n",
      "长度 48: 出现次数 109\n",
      "长度 49: 出现次数 103\n",
      "长度 50: 出现次数 95\n",
      "长度 51: 出现次数 124\n",
      "长度 52: 出现次数 129\n",
      "长度 53: 出现次数 129\n",
      "长度 54: 出现次数 112\n",
      "长度 55: 出现次数 130\n",
      "长度 56: 出现次数 143\n",
      "长度 57: 出现次数 128\n",
      "长度 58: 出现次数 133\n",
      "长度 59: 出现次数 142\n",
      "长度 60: 出现次数 131\n",
      "长度 61: 出现次数 134\n",
      "长度 62: 出现次数 126\n",
      "长度 63: 出现次数 158\n",
      "长度 64: 出现次数 148\n",
      "长度 65: 出现次数 136\n",
      "长度 66: 出现次数 140\n",
      "长度 67: 出现次数 133\n",
      "长度 68: 出现次数 142\n",
      "长度 69: 出现次数 129\n",
      "长度 70: 出现次数 148\n",
      "长度 71: 出现次数 112\n",
      "长度 72: 出现次数 139\n",
      "长度 73: 出现次数 156\n",
      "长度 74: 出现次数 145\n",
      "长度 75: 出现次数 122\n",
      "长度 76: 出现次数 137\n",
      "长度 77: 出现次数 155\n",
      "长度 78: 出现次数 125\n",
      "长度 79: 出现次数 138\n",
      "长度 80: 出现次数 150\n",
      "长度 81: 出现次数 145\n",
      "长度 82: 出现次数 126\n",
      "长度 83: 出现次数 162\n",
      "长度 84: 出现次数 129\n",
      "长度 85: 出现次数 148\n",
      "长度 86: 出现次数 136\n",
      "长度 87: 出现次数 162\n",
      "长度 88: 出现次数 152\n",
      "长度 89: 出现次数 146\n",
      "长度 90: 出现次数 146\n",
      "长度 91: 出现次数 137\n",
      "长度 92: 出现次数 124\n",
      "长度 93: 出现次数 154\n",
      "长度 94: 出现次数 147\n",
      "长度 95: 出现次数 127\n",
      "长度 96: 出现次数 166\n",
      "长度 97: 出现次数 117\n",
      "长度 98: 出现次数 160\n",
      "长度 99: 出现次数 163\n",
      "长度 100: 出现次数 119\n",
      "长度 101: 出现次数 146\n",
      "长度 102: 出现次数 128\n",
      "长度 103: 出现次数 103\n",
      "长度 104: 出现次数 152\n",
      "长度 105: 出现次数 122\n",
      "长度 106: 出现次数 143\n",
      "长度 107: 出现次数 116\n",
      "长度 108: 出现次数 128\n",
      "长度 109: 出现次数 125\n",
      "长度 110: 出现次数 133\n",
      "长度 111: 出现次数 123\n",
      "长度 112: 出现次数 91\n",
      "长度 113: 出现次数 133\n",
      "长度 114: 出现次数 95\n",
      "长度 115: 出现次数 116\n",
      "长度 116: 出现次数 124\n",
      "长度 117: 出现次数 120\n",
      "长度 118: 出现次数 104\n",
      "长度 119: 出现次数 119\n",
      "长度 120: 出现次数 122\n",
      "长度 121: 出现次数 114\n",
      "长度 122: 出现次数 125\n",
      "长度 123: 出现次数 101\n",
      "长度 124: 出现次数 90\n",
      "长度 125: 出现次数 106\n",
      "长度 126: 出现次数 129\n",
      "长度 127: 出现次数 94\n",
      "长度 128: 出现次数 103\n",
      "长度 129: 出现次数 111\n",
      "长度 130: 出现次数 94\n",
      "长度 131: 出现次数 98\n",
      "长度 132: 出现次数 79\n",
      "长度 133: 出现次数 89\n",
      "长度 134: 出现次数 85\n",
      "长度 135: 出现次数 95\n",
      "长度 136: 出现次数 63\n",
      "长度 137: 出现次数 72\n",
      "长度 138: 出现次数 83\n",
      "长度 139: 出现次数 84\n",
      "长度 140: 出现次数 86\n",
      "长度 141: 出现次数 73\n",
      "长度 142: 出现次数 87\n",
      "长度 143: 出现次数 65\n",
      "长度 144: 出现次数 68\n",
      "长度 145: 出现次数 59\n",
      "长度 146: 出现次数 71\n",
      "长度 147: 出现次数 38\n",
      "长度 148: 出现次数 63\n",
      "长度 149: 出现次数 76\n",
      "长度 150: 出现次数 67\n",
      "长度 151: 出现次数 61\n",
      "长度 152: 出现次数 62\n",
      "长度 153: 出现次数 65\n",
      "长度 154: 出现次数 46\n",
      "长度 155: 出现次数 67\n",
      "长度 156: 出现次数 63\n",
      "长度 157: 出现次数 59\n",
      "长度 158: 出现次数 64\n",
      "长度 159: 出现次数 45\n",
      "长度 160: 出现次数 60\n",
      "长度 161: 出现次数 57\n",
      "长度 162: 出现次数 51\n",
      "长度 163: 出现次数 58\n",
      "长度 164: 出现次数 55\n",
      "长度 165: 出现次数 45\n",
      "长度 166: 出现次数 60\n",
      "长度 167: 出现次数 41\n",
      "长度 168: 出现次数 58\n",
      "长度 169: 出现次数 39\n",
      "长度 170: 出现次数 45\n",
      "长度 171: 出现次数 35\n",
      "长度 172: 出现次数 44\n",
      "长度 173: 出现次数 44\n",
      "长度 174: 出现次数 38\n",
      "长度 175: 出现次数 43\n",
      "长度 176: 出现次数 47\n",
      "长度 177: 出现次数 46\n",
      "长度 178: 出现次数 40\n",
      "长度 179: 出现次数 41\n",
      "长度 180: 出现次数 32\n",
      "长度 181: 出现次数 39\n",
      "长度 182: 出现次数 41\n",
      "长度 183: 出现次数 35\n",
      "长度 184: 出现次数 51\n",
      "长度 185: 出现次数 34\n",
      "长度 186: 出现次数 40\n",
      "长度 187: 出现次数 31\n",
      "长度 188: 出现次数 50\n",
      "长度 189: 出现次数 35\n",
      "长度 190: 出现次数 44\n",
      "长度 191: 出现次数 19\n",
      "长度 192: 出现次数 45\n",
      "长度 193: 出现次数 28\n",
      "长度 194: 出现次数 20\n",
      "长度 195: 出现次数 37\n",
      "长度 196: 出现次数 22\n",
      "长度 197: 出现次数 33\n",
      "长度 198: 出现次数 32\n",
      "长度 199: 出现次数 33\n",
      "长度 200: 出现次数 29\n",
      "长度 201: 出现次数 32\n",
      "长度 202: 出现次数 24\n",
      "长度 203: 出现次数 31\n",
      "长度 204: 出现次数 22\n",
      "长度 205: 出现次数 26\n",
      "长度 206: 出现次数 26\n",
      "长度 207: 出现次数 17\n",
      "长度 208: 出现次数 25\n",
      "长度 209: 出现次数 19\n",
      "长度 210: 出现次数 20\n",
      "长度 211: 出现次数 9\n",
      "长度 212: 出现次数 21\n",
      "长度 213: 出现次数 19\n",
      "长度 214: 出现次数 19\n",
      "长度 215: 出现次数 14\n",
      "长度 216: 出现次数 17\n",
      "长度 217: 出现次数 20\n",
      "长度 218: 出现次数 31\n",
      "长度 219: 出现次数 15\n",
      "长度 220: 出现次数 22\n",
      "长度 221: 出现次数 19\n",
      "长度 222: 出现次数 15\n",
      "长度 223: 出现次数 8\n",
      "长度 224: 出现次数 17\n",
      "长度 225: 出现次数 14\n",
      "长度 226: 出现次数 21\n",
      "长度 227: 出现次数 13\n",
      "长度 228: 出现次数 18\n",
      "长度 229: 出现次数 15\n",
      "长度 230: 出现次数 13\n",
      "长度 231: 出现次数 30\n",
      "长度 232: 出现次数 14\n",
      "长度 233: 出现次数 14\n",
      "长度 234: 出现次数 12\n",
      "长度 235: 出现次数 17\n",
      "长度 236: 出现次数 13\n",
      "长度 237: 出现次数 14\n",
      "长度 238: 出现次数 17\n",
      "长度 239: 出现次数 8\n",
      "长度 240: 出现次数 14\n",
      "长度 241: 出现次数 20\n",
      "长度 242: 出现次数 16\n",
      "长度 243: 出现次数 11\n",
      "长度 244: 出现次数 6\n",
      "长度 245: 出现次数 11\n",
      "长度 246: 出现次数 11\n",
      "长度 247: 出现次数 10\n",
      "长度 248: 出现次数 13\n",
      "长度 249: 出现次数 9\n",
      "长度 250: 出现次数 10\n",
      "长度 251: 出现次数 3\n",
      "长度 252: 出现次数 10\n",
      "长度 253: 出现次数 6\n",
      "长度 254: 出现次数 13\n",
      "长度 255: 出现次数 6\n",
      "长度 256: 出现次数 5\n",
      "长度 257: 出现次数 14\n",
      "长度 258: 出现次数 10\n",
      "长度 259: 出现次数 17\n",
      "长度 260: 出现次数 14\n",
      "长度 261: 出现次数 6\n",
      "长度 262: 出现次数 9\n",
      "长度 263: 出现次数 7\n",
      "长度 264: 出现次数 16\n",
      "长度 265: 出现次数 8\n",
      "长度 266: 出现次数 10\n",
      "长度 267: 出现次数 10\n",
      "长度 268: 出现次数 6\n",
      "长度 269: 出现次数 5\n",
      "长度 270: 出现次数 6\n",
      "长度 271: 出现次数 7\n",
      "长度 272: 出现次数 18\n",
      "长度 273: 出现次数 6\n",
      "长度 274: 出现次数 5\n",
      "长度 275: 出现次数 12\n",
      "长度 276: 出现次数 13\n",
      "长度 277: 出现次数 5\n",
      "长度 278: 出现次数 7\n",
      "长度 279: 出现次数 11\n",
      "长度 280: 出现次数 7\n",
      "长度 281: 出现次数 7\n",
      "长度 282: 出现次数 11\n",
      "长度 283: 出现次数 9\n",
      "长度 284: 出现次数 6\n",
      "长度 285: 出现次数 3\n",
      "长度 286: 出现次数 5\n",
      "长度 287: 出现次数 13\n",
      "长度 288: 出现次数 6\n",
      "长度 289: 出现次数 7\n",
      "长度 290: 出现次数 5\n",
      "长度 291: 出现次数 9\n",
      "长度 292: 出现次数 16\n",
      "长度 293: 出现次数 8\n",
      "长度 294: 出现次数 8\n",
      "长度 295: 出现次数 5\n",
      "长度 296: 出现次数 6\n",
      "长度 297: 出现次数 1\n",
      "长度 298: 出现次数 10\n",
      "长度 299: 出现次数 7\n",
      "长度 300: 出现次数 7\n",
      "长度 301: 出现次数 11\n",
      "长度 302: 出现次数 8\n",
      "长度 304: 出现次数 11\n",
      "长度 305: 出现次数 2\n",
      "长度 306: 出现次数 3\n",
      "长度 307: 出现次数 4\n",
      "长度 308: 出现次数 7\n",
      "长度 309: 出现次数 7\n",
      "长度 310: 出现次数 1\n",
      "长度 311: 出现次数 7\n",
      "长度 312: 出现次数 7\n",
      "长度 313: 出现次数 5\n",
      "长度 314: 出现次数 4\n",
      "长度 315: 出现次数 6\n",
      "长度 316: 出现次数 5\n",
      "长度 317: 出现次数 5\n",
      "长度 318: 出现次数 5\n",
      "长度 319: 出现次数 3\n",
      "长度 320: 出现次数 3\n",
      "长度 321: 出现次数 7\n",
      "长度 322: 出现次数 4\n",
      "长度 323: 出现次数 3\n",
      "长度 324: 出现次数 2\n",
      "长度 325: 出现次数 6\n",
      "长度 326: 出现次数 5\n",
      "长度 327: 出现次数 6\n",
      "长度 328: 出现次数 6\n",
      "长度 329: 出现次数 3\n",
      "长度 330: 出现次数 2\n",
      "长度 331: 出现次数 2\n",
      "长度 332: 出现次数 1\n",
      "长度 334: 出现次数 3\n",
      "长度 335: 出现次数 2\n",
      "长度 336: 出现次数 2\n",
      "长度 337: 出现次数 1\n",
      "长度 338: 出现次数 4\n",
      "长度 339: 出现次数 2\n",
      "长度 340: 出现次数 5\n",
      "长度 341: 出现次数 3\n",
      "长度 342: 出现次数 4\n",
      "长度 343: 出现次数 1\n",
      "长度 344: 出现次数 2\n",
      "长度 345: 出现次数 1\n",
      "长度 346: 出现次数 1\n",
      "长度 347: 出现次数 4\n",
      "长度 348: 出现次数 1\n",
      "长度 349: 出现次数 4\n",
      "长度 350: 出现次数 4\n",
      "长度 351: 出现次数 2\n",
      "长度 352: 出现次数 3\n",
      "长度 353: 出现次数 1\n",
      "长度 354: 出现次数 3\n",
      "长度 355: 出现次数 5\n",
      "长度 356: 出现次数 1\n",
      "长度 357: 出现次数 2\n",
      "长度 358: 出现次数 3\n",
      "长度 361: 出现次数 2\n",
      "长度 362: 出现次数 4\n",
      "长度 363: 出现次数 3\n",
      "长度 365: 出现次数 1\n",
      "长度 367: 出现次数 1\n",
      "长度 368: 出现次数 1\n",
      "长度 370: 出现次数 2\n",
      "长度 371: 出现次数 7\n",
      "长度 372: 出现次数 7\n",
      "长度 373: 出现次数 2\n",
      "长度 374: 出现次数 4\n",
      "长度 375: 出现次数 1\n",
      "长度 377: 出现次数 4\n",
      "长度 378: 出现次数 7\n",
      "长度 379: 出现次数 8\n",
      "长度 380: 出现次数 1\n",
      "长度 381: 出现次数 4\n",
      "长度 382: 出现次数 2\n",
      "长度 383: 出现次数 2\n",
      "长度 384: 出现次数 1\n",
      "长度 385: 出现次数 1\n",
      "长度 386: 出现次数 1\n",
      "长度 388: 出现次数 1\n",
      "长度 389: 出现次数 2\n",
      "长度 390: 出现次数 2\n",
      "长度 391: 出现次数 3\n",
      "长度 393: 出现次数 2\n",
      "长度 394: 出现次数 2\n",
      "长度 395: 出现次数 1\n",
      "长度 396: 出现次数 3\n",
      "长度 399: 出现次数 1\n",
      "长度 400: 出现次数 1\n",
      "长度 401: 出现次数 1\n",
      "长度 403: 出现次数 3\n",
      "长度 404: 出现次数 1\n",
      "长度 405: 出现次数 1\n",
      "长度 406: 出现次数 1\n",
      "长度 409: 出现次数 2\n",
      "长度 410: 出现次数 3\n",
      "长度 411: 出现次数 3\n",
      "长度 412: 出现次数 4\n",
      "长度 413: 出现次数 2\n",
      "长度 414: 出现次数 2\n",
      "长度 416: 出现次数 1\n",
      "长度 417: 出现次数 2\n",
      "长度 423: 出现次数 2\n",
      "长度 426: 出现次数 2\n",
      "长度 427: 出现次数 1\n",
      "长度 428: 出现次数 1\n",
      "长度 429: 出现次数 1\n",
      "长度 430: 出现次数 1\n",
      "长度 433: 出现次数 1\n",
      "长度 434: 出现次数 1\n",
      "长度 437: 出现次数 2\n",
      "长度 438: 出现次数 3\n",
      "长度 441: 出现次数 1\n",
      "长度 442: 出现次数 4\n",
      "长度 451: 出现次数 1\n",
      "长度 454: 出现次数 1\n",
      "长度 461: 出现次数 1\n",
      "长度 462: 出现次数 1\n",
      "长度 472: 出现次数 1\n",
      "长度 474: 出现次数 1\n",
      "长度 476: 出现次数 1\n",
      "长度 477: 出现次数 3\n",
      "长度 481: 出现次数 1\n",
      "长度 486: 出现次数 2\n",
      "长度 487: 出现次数 1\n",
      "长度 492: 出现次数 1\n",
      "长度 497: 出现次数 1\n",
      "长度 498: 出现次数 1\n",
      "长度 502: 出现次数 1\n",
      "长度 510: 出现次数 2\n",
      "长度 515: 出现次数 1\n",
      "长度 516: 出现次数 1\n",
      "长度 519: 出现次数 1\n",
      "长度 522: 出现次数 1\n",
      "长度 531: 出现次数 1\n",
      "长度 532: 出现次数 5\n",
      "长度 535: 出现次数 1\n",
      "长度 537: 出现次数 1\n",
      "长度 538: 出现次数 1\n",
      "长度 539: 出现次数 1\n",
      "长度 542: 出现次数 1\n",
      "长度 545: 出现次数 3\n",
      "长度 569: 出现次数 1\n",
      "长度 581: 出现次数 1\n",
      "长度 595: 出现次数 2\n",
      "长度 607: 出现次数 1\n",
      "长度 613: 出现次数 1\n",
      "长度 614: 出现次数 1\n",
      "长度 615: 出现次数 1\n",
      "长度 620: 出现次数 1\n",
      "长度 627: 出现次数 2\n",
      "长度 628: 出现次数 1\n",
      "长度 632: 出现次数 1\n",
      "长度 655: 出现次数 1\n",
      "长度 663: 出现次数 1\n",
      "长度 676: 出现次数 1\n",
      "长度 692: 出现次数 1\n",
      "长度 749: 出现次数 1\n",
      "长度 771: 出现次数 1\n",
      "长度 792: 出现次数 2\n",
      "长度 837: 出现次数 1\n",
      "长度 922: 出现次数 1\n",
      "长度 1457: 出现次数 1\n",
      "5595\n"
     ]
    }
   ],
   "source": [
    "# 计算每个字符串的长度\n",
    "lengths = [len(text) for text in test_texts]\n",
    "\n",
    "# 计算平均长度\n",
    "average_length = sum(lengths) / len(lengths)\n",
    "\n",
    "# 找到最短和最长的字符串长度\n",
    "min_length = min(lengths)\n",
    "max_length = max(lengths)\n",
    "\n",
    "# 生成长度的频率分布\n",
    "from collections import Counter\n",
    "length_distribution = Counter(lengths)\n",
    "\n",
    "print(f\"平均长度: {average_length}\")\n",
    "print(f\"最短长度: {min_length}, 最长长度: {max_length}\")\n",
    "print(\"长度频率分布:\")\n",
    "beyond = 0\n",
    "for length, count in sorted(length_distribution.items()):\n",
    "    print(f\"长度 {length}: 出现次数 {count}\")\n",
    "    if length > 128:\n",
    "        beyond += count\n",
    "print(beyond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切割函数\n",
    "def split_text(texts, batchsize=128):\n",
    "    split_texts = []    # 存放切割后的文本段落\n",
    "    sign = []           # 存放每个文本段落的标记，以便后面合并分割的段落\n",
    "    count = 0           # 用于记录当前文本段落的索引\n",
    "    for text in texts:\n",
    "        if len(text) > batchsize:\n",
    "            num = int(len(text) / batchsize)\n",
    "            # 如果文本长度超过了指定的批次大小，则进行切割\n",
    "            for i in range(num):\n",
    "                # 将文本按照批次大小进行切割，添加到切割后的文本列表中\n",
    "                split_texts.append(text[batchsize * i: batchsize * (i + 1)])\n",
    "                # 记录当前文本段落的标记，方便后面合并分割的段落\n",
    "                sign.append(count)\n",
    "            # 将剩余的部分作为最后一个段落\n",
    "            split_texts.append(text[batchsize * num:])\n",
    "            sign.append(count)\n",
    "        else:\n",
    "            # 如果文本长度未超过批次大小，则不进行切割，直接添加到切割后的文本列表中\n",
    "            split_texts.append(text)\n",
    "            sign.append(count)\n",
    "        \n",
    "        count += 1  # 更新文本段落的索引\n",
    "    \n",
    "    return split_texts, sign\n",
    "\n",
    "# 使用函数切割文本\n",
    "split_test_texts, sign = split_text(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts          # 保存传入的文本数据列表\n",
    "        self.tokenizer = tokenizer  # 保存分词器对象\n",
    "        self.max_len = max_len      # 设置模型输入的最大序列长度\n",
    "\n",
    "    # 返回数据集中文本的数量\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # 通过索引获取数据集中的一个样本\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  # 根据索引获取单个文本字符串\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',   # 不足max_len的部分使用pad填充\n",
    "            truncation=True,        # 超过max_len的部分进行截断\n",
    "            max_length=self.max_len,  # 设置最大长度\n",
    "            return_tensors='pt'         # 返回pytorch张量\n",
    "        )\n",
    "        \n",
    "\n",
    "        # input_ids: 编码后的token ID序列 表示文本的数字形式\n",
    "        # attention_mas': 注意力掩码 与input_ids长度相同\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'][0],\n",
    "            'attention_mask': encoding['attention_mask'][0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定之前保存的目录路径\n",
    "saved_tokenizer_directory = './trains/train1/bert_tokenizer/'\n",
    "saved_model_directory = './trains/train1/bert_model/'\n",
    "\n",
    "# 重新加载分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(saved_tokenizer_directory)\n",
    "\n",
    "# 重新加载模型\n",
    "model = BertForTokenClassification.from_pretrained(saved_model_directory)\n",
    "\n",
    "# 现在，tokenizer和model已经加载完毕，可以用于文本处理和推理等任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(split_test_texts, tokenizer, max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def tag_test(model, test_dataset, device):\n",
    "    '''对 test.txt 进行序列标注，并显示测试进度'''\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # 验证时不需要shuffle\n",
    "\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    model.to(device)\n",
    "\n",
    "    # 存放所有的句子的结果\n",
    "    result = []\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        # 使用tqdm包装数据加载器以显示进度\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\", unit=\"batch\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # 模型前向传播\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # 获取概率分布并预测类别\n",
    "            prob = F.softmax(outputs.logits, dim=-1)\n",
    "            preds = torch.argmax(prob, dim=-1)\n",
    "\n",
    "            # 收集预测结果\n",
    "            result.extend(preds.tolist())  # 直接扩展列表，无需内部循环\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "results = tag_test(model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32843, 32843, 32843)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results), len(split_test_texts), len(sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx['pad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个空列表，用于存储过滤后的结果\n",
    "filtered_result = []\n",
    "\n",
    "# 遍历原始结果列表中的每个样本\n",
    "for result in results:\n",
    "    # 遍历当前样本中的每个元素 去除pad对应的索引\n",
    "    f_result = [num for num in result if num != label2idx['pad']]\n",
    "    filtered_result.append(f_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32843"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26264\n"
     ]
    }
   ],
   "source": [
    "def merge_elements(filtered_result, sign):\n",
    "    # 初始化一个字典来存储合并后的结果\n",
    "    merged_dict = {}\n",
    "    # 遍历sign和filtered_result，根据sign的值合并filtered_result的元素\n",
    "    for index, value in enumerate(sign):\n",
    "        if value not in list(merged_dict.keys()):\n",
    "            # 如果sign的值首次出现，直接赋值\n",
    "            merged_dict[value] = filtered_result[index]\n",
    "        else:\n",
    "\n",
    "            # 如果sign的值重复出现，合并filtered_result的元素（这里假设是列表相加）\n",
    "            merged_dict[value].extend(filtered_result[index])\n",
    "    \n",
    "    # 确保sign中的值排序与原列表一致，然后根据这个顺序重组merged_result\n",
    "    sorted_sign = sorted(merged_dict.keys())\n",
    "    merged_result = [merged_dict[val] for val in sorted_sign]\n",
    "    \n",
    "    return merged_result\n",
    "\n",
    "# 调用函数\n",
    "merged_list = merge_elements(filtered_result, sign)\n",
    "print(len(merged_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listTotags(merged_list):\n",
    "    '''将列表中的数字转化为tags标签'''\n",
    "    merged_tags = []\n",
    "    for sentence in merged_list:\n",
    "        tags = [tag_set[num] for num in sentence]\n",
    "        # 将一个长文本的标签合并到一起 中间用空格隔开\n",
    "        tags = ' '.join(tags)\n",
    "        merged_tags.append(tags)\n",
    "\n",
    "    return merged_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tags = listTotags(merged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O O B_T I_T I_T I_T I_T O B_LOC I_LOC O O O B_LOC I_LOC O O O O O B_LOC I_LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O B_T I_T I_T I_T O O O O O O O O O O O O B_LOC I_LOC O O O B_LOC I_LOC O O B_LOC I_LOC O O O O O O B_LOC I_LOC O O O O O O O O O O O O O O O O O O'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'记者1月15日从上海铁路局淮南西站获悉,淮南铁路预计春运期间发送旅客33.3万人。预计客流最高峰日为2月6日,将发送旅客1.8万人,淮南东开往广州南及北京方向的高铁、淮南站开往沪杭甬方向的列车将是热门车次。'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2021213346.txt', 'w') as file:\n",
    "    # 遍历列表，将每个元素写入文件，每个元素后跟一个换行符('\\n')\n",
    "    for tag in merged_tags:\n",
    "        file.write(tag + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
